% Machine Learning cheat sheet
%% (compile with XeTeX. see notes in structure_cheatSheet.tex if no XeTeX.)

\documentclass[11pt]{article}
\synctex=1
\input{structure_cheatSheet.tex}
\addbibresource{refs_cheatSheet.bib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{ML Cheatsheet}
\author{Steve Young}
\abstract{Everything I know about machine learning.}

\begin{document}
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conventions}
\label{sec:conv}
\paragraph{Math Notation}
\begin{itemize}
  \item $x \in \{0, 1\}^r$ : $x$ is a vector of form \eeg $(0, 1, 1, 0, \dots, 1, 0)$ of
  length $r$.
  \item $\sim$ : random variable drawn from a distribution\\
  $\propto$ : ``proportional to''
  \item $\mbf{1}$($\cdot$) : indicator function --- 1 when arg is true, 0 when arg is
  false.
  \item $\mbf{I}_k$: $k \times k$ identity matrix
  \item Boldface capital letters (roman or greek) are matrices, \eeg $\mA = \mU \mLambda
  \mV^T$ 
  \item $\log$ is base $e$ by default $\to$ entropy in nats.
  \item $\E_{\theta}$ : Expectation value (with respect to some parameter
  $\theta$. Parameter may be omitted if clear from context.)
\end{itemize}

\paragraph{Ng CS229 / DeepLearning.ai}
\begin{itemize}
  \item $m$ : number of training examples in the dataset
  \item $n$ : dimension of training examples
  \item $x^{(i)} \in \mbb{R}^{n}$ : $i^{th}$ training example (\emph{column} vector),
  $1 \leq i \leq m$
  \item $y^{(i)}$ : $i^{th}$ output (\emph{column} vector), $1 \leq i \leq m$
  \item $x_j$ : $j^{th}$ component of a training example, $1 \leq j \leq n$.
  \item $\mX \in \mbb{R}^{m \times n}$ : (input/design) matrix (training examples are
  \emph{row} vectors)
\end{itemize}

\paragraph{\cite{BISHOP}}
\begin{itemize}
  \item $N$ : number of training examples in the dataset
  \item $D$ : dimension of training examples
  \item $M$ : number of basis functions
  \item Boldface lower-case letters (roman or greek) are \emph{column} vectors, \eeg
  $d = \mbf{x}^T \mA \mbf{x}$, or $\mbf{y} = \mA \mbf{x}$.
  \item $\bms{\phi}$: $M$-dim basis function
  \item $\mPhi \in \mathbb{R}^{N \times M}$: Design matrix (basis functions are
  \emph{row} vectors)
\end{itemize}


\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Linear Regression}
\label{sec:linreg}
\subsection{Basics}
\begin{itemize}
  \item Hypothesis
  $h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots = \sum_{j=0}^{n}
  \theta_j x_j \equiv \theta^T x$, where $x_0 = 1$.
  \item Cost function
  \begin{equation}
    J(\theta) = \frac{1}{2} \sum_{i=1}^m \left(h_\theta(x^{(i)}) - y^{(i)}\right)^2
  \end{equation}
  \item Analytically solve via normal equations, where $\mX$ is the augmented
  $m \times (n+1)$ design matrix
  \begin{equation}
    \v{\theta} = \left(\mX^T \mX \right)^{-1} \mX^T \, \v{y}
  \end{equation}
  where $\v{\theta}$ and $\v{y}$ are column vectors of the $n+1$ weights and $m$
  outputs, respectively.
\end{itemize}

\subsection{Analysis}
\TODO{write in explanatory defs of things like $R^2$ and the other relevant linear
  regression metrics.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Logistic Regression}
\label{sec:logreg}
\subsection{2 classes}
With 2 labels $y \in \{0,1\}$ and $m$ training examples $x^{(i)},\ \ 1 \leq i \leq m$,
we have
\begin{equation}
  h_\theta (x) = \frac{1}{1 + e^{-\theta^T x}}
\end{equation}
\begin{equation}
  p(y|x;\theta) = (h_\theta (x))^y (1 - h_\theta (x))^{1-y}
\end{equation}
\begin{equation}
  L(\theta) = \prod_{i=1}^m (h_\theta (x^{(i)}))^{y^{(i)}} (1 - h_\theta
  (x^{(i)}))^{1-y^{(i)}}
\end{equation}
\begin{equation}
  l(\theta) = \log L(\theta) = \sum_{i=1}^m y^{(i)} \log h_\theta (x^{(i)}) + (1 -
  y^{(i)}) \log(1 - h_\theta  (x^{(i)}))
\end{equation}
with deriv
\begin{equation}
  \frac{\partial}{\partial \theta_j} l(\theta) = x_j^T (y - h_\theta (x))
\end{equation}
We can't minimize the log-likelihood analytically, so we must use numerical
optimization.

The log-likelihood is just the cross entropy
\begin{equation}
  H(p,q) = -\sum_i p_i \log q_i
\end{equation}
with $p$ the actual outputs, and $q$ the hypothesis $h_\theta(x)$

\subsection{2+ classes}

\tbf{One-versus-all:} For $k$ classes, do $k$ normal logistic regressions. Each has two
classes: the target class, and all the rest. New examples are classified by whichever of
these $k$ logistic regressions has highest score.

\tbf{Softmax regression:} Model classification cases as \emph{multinomial}
distribution. For $k$ classes, hypothesis is $k$-dim vector
\begin{equation}
  h_\theta (x) = \frac{1}{\sum_{l=1}^k \exp(\theta^{(l)T} x)} \times
  \big[\exp(\theta^{(1)T} x), \dots, \exp(\theta^{(k)T} x)\big] 
\end{equation}
so that the final output layer has $k$ units. The log-likelihood is
\begin{equation}
  l(\theta) = - \sum_{i=1}^m \sum_{l=1}^k \mbf{1}(y^{(i)} = l)
  \log \frac{\exp(\theta^{(l)T} x)}{\sum_{m=1}^k \exp(\theta^{(m)T} x)} 
\end{equation}

\TODO{show how 2-class logistic regression cost func can be written this way. write down
  deriv of cost func. clean up notation.}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Gaussian Discriminant Analysis (GDA) and Naive Bayes}
\label{sec:GDA_nbayes}
\tbfit{Generative algorithm}: Instead of modeling $p(y|x)$, model $p(y)$ and $p(x|y)$,
then get posterior $p(y|x)$ via Bayes' theorem.

\subsection{GDA}
Can use GDA for classification problem when input features are continuous-val random
vars. Models $p(x|y)$ as multivariate Gaussian. Model is
\begin{align}
  y     &\sim \trm{Bernoulli}(\phi) \nncr
  x|y=0 &\sim \mathcal{N(\mu_0, \mSigma)} \nncr
  x|y=1 &\sim \mathcal{N(\mu_1, \mSigma)}
\end{align}

\TODOFIN{}

\subsection{Naive Bayes (for text classification)}
For training examples $x \in \{0,1\}^n$ (a \tbfit{vocabulary}\footnote{the $j^{th}$
  entry is 1 if the example contains the $j^{th}$ word in the vocabulary. \tbf{Example}:
  If the vocab is \{cats, rats, bats\}, then $n=3$, and a training example that contains
  ``cats'', ``rats'', but not ``bats'', would be $x = (1, 1, 0)$.} of length $n$),
assume the components of an example, $x_j$, are conditionally independent given $y$
(\tbfit{Naive Bayes assumption}).
\begin{equation}
  p(x_1, \dots, x_n | y) = \prod_{i=1}^n p(x_i | y)
\end{equation}

Model is parameterized by
\begin{equation}
  \phi_y = p(y=1),\qquad \phi_{j|y=1} = p(x_j=1|y=1), \qquad \phi_{j|y=0} = p(x_j=1|y=0)
\end{equation}
where $j \in (1, n)$, for a total of $2n +1$ parameters. Likelihood is
\begin{equation}
  L(\phi_y, \{\phi_{j|y=1}, \phi_{j|y=0}\}_{j=1}^n) = \prod_{i=1}^m p(x^{(i)}, y^{(i)})
\end{equation}
which has MLE values
\begin{align}
  \phi_{j|y=1} &= \trm{fraction of spam (y=1) in which word j appears}\nncr
  \phi_{j|y=0} &= \trm{fraction of non-spam (y=0) in which word j appears}\nncr
  \phi_{y}     &= \trm{ fraction of training examples that are spam}
\end{align}
To make predictions, we don't need the evidence $p(x)$; we just need to compare
\begin{align}
  p(y=1|x) &\propto p(x|y=1)p(y=1)\nncr
  p(y=0|x) &\propto p(x|y=0)p(y=0)
\end{align}
and pick the class that has the higher value (un-normalized posterior).

\TODOFIN{show how last two eqs are actually calculated}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bias/Variance Tradeoff}
\label{sec:bias_var}
\begin{itemize}
  \item \tbf{High bias}: underfitting. high training error and test error.
  \item \tbf{High variance}: overfitting. \tit{low} training error and
  \tit{high} test error
\end{itemize}

Can't use the test set to decide on values of hypers (\eeg number of parameters in
model) because ``we could just tweak the values of the hypers until the estimator
performs optimally [on the test set]''.

Cross-validation allows you to do model comparison to find best value of hypers (on
frequentist stats), versus using things like computing the evidence in Bayesian
stats. \LATODO{more detail here}

Can average together multiple models with high capacity (low bias, but high
variance). The result of combining is to \emph{reduce} the variance of the combined
model.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Model Assessment}
\label{sec:model_ass}
\subsection{Classification}
\subsubsection{Precision/Recall}
For 2 classes, with \{TN, FN, FP, TP\} = \{true negatives, false negatives, false
positives, true positives\}, we have the \tbfit{loss} or \tbfit{confusion matrix}:
\begin{equation}
  \lp
    \begin{matrix}
      \trm{TN} & \trm{FP}\\
      \trm{FN} & \trm{TP}
    \end{matrix}
  \rp,
\end{equation}
where the rows/columns are actual/predicted numbers of examples not-in-class (negative),
or in-class (positive).

We define the \tbfit{precision} and \tbfit{recall}:
\begin{equation}
  \trm{precision} = \frac{\trm{TP}}{\trm{TP} + \trm{FP}}, \qquad
  \trm{recall} = \frac{\trm{TP}}{\trm{TP} + \trm{FN}}.
\end{equation}
\tbfit{Precision} is ability of classifier to not classify actual negatives as positive
(\eie \emph{low} false positives). So \eeg if positive is ``person doesn't have
cancer'', we want \emph{high precision}.

\tbfit{Recall} is ability of classifier to find all positive samples.

\subsubsection{Receiver operating characteristic curve (ROC)}
\tbfit{True Positive Rate (TPR):} same as recall \\
\tbfit{True Negative Rate (TNR):} fraction of actual negatives that are classified as
negative $\trm{TN}/(\trm{TN} + \trm{FP})$
\tbfit{False Positive Rate (FPR):} $1 - \trm{TNR} = \trm{FP}/(\trm{TN} + \trm{FP})$ 

ROC plots TPR versus FPR.

\tbfit{Area under ROC Curve (AUC/AUC-ROC)} is useful mesaure of performace across all
classification thresholds. Equal to probability that classifier will \emph{rank a random
  positive sample higher than a random negative sample.}

\TODO{More on why the ROC is useful. Maybe a picture.}

\subsubsection{F1 Score}
The F1 score is the \emph{harmonic mean} of precision and recall. It's an ok one-number
metric for evaluating effectiveness of different classifiers on problem:
\begin{equation}
  \label{eq:F1}
  \trm{F1} = \frac{2}{\left(\frac{1}{\trm{prec}} + \frac{1}{\trm{rec}}\right)}
\end{equation}

\subsection{Regression}
\TODO{Write this section.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{K-means}
\label{sec:kmeans}
\begin{itemize}
  \item Setup: Select any $K$ points from data to serve as initial centroids of the
  clusters.
  \item Repeat until (stopping criteria: \eeg no points change clusters, sum of
  distances is minimized, some total number of iterations..):
  \begin{enumerate}
    \item Assign each datapoint to the cluster with closest centroid.
    \item Compute $K$ new centroids, each the mean of the datapoints in its cluster.
  \end{enumerate}
\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{PCA}
\label{sec:pca}
PCA is unsupervised technique for finding directions of most variance in dataset. The
\tbfit{principal components} are a set of $n$ orthonormal $n$-dim basis vectors along
the directions of maximum variance, ordered by decreasing variance in their directions.

\tbf{To calc:}
\begin{itemize}
  \item \tbf{via eigenvectors:} Principal components of \emph{de-meaned} $m \times n$
  design matrix $\mX$ are the eigenvectors of the covariance matrix of $\mX$,
  $\mbf{C_X} = \frac{1}{m} \mX^T \mX$.
  \item \tbf{via SVD:} Define $\mY = \frac{1}{\sqrt{m}} \mX$, and take its SVD
  \ref{subsubsec:SVD}, which is $\mY = \mU \mW \mV^T$. Then $\mU^T$ is $n \times m$
  matrix that projects $\mX$ onto its principal components, $\mW$ is the matrix of
  principal comp values, and $\mV$ is orthonormal matrix of the principal
  components. \TODO{Verify/fix statement on $\mU$ projection.}
\end{itemize}


\TODO{re-check everything in PCA section, as I have redefined the design matrix $\mX$.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Time Series}
\label{sec:timeseries}
\subsection{Stationarity}

\begin{definition}[Second Order Stationarity] is when correlation between sequential
  observations is only a function of the lag.
\end{definition}

\begin{itemize}
  \item \tbf{Dickey-Fuller test:} tests for stationarity of AR model. Null hypothesis is
  ``series is \tit{non-stationary}''.
\end{itemize}

\subsection{ARCH}
\begin{itemize}
  \item{\tbf{Box-Jenkins:} systematic methodology for identifying and estimating models
    that can incorporate both AR and MA.}
\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Gaussian Processes}
\label{sec:gp}
\subsection{Prior over functions, from basis functions}
Following the treatment in section 6.4 of \cite{BISHOP} (with Bishop's notation).

Let $d$ be the number of \tbfit{basis functions} $\v{\phi}$, which are non-linear
functions of the $n$-dim training examples $\v{x}$. We write the hypothesis as
\begin{equation}
  y(\v{x}) = \v{w}^T \v{\phi}(\v{x})
\end{equation}
where $\v{w}$ is a $d$-dim vector of weights.

As in MAP and Bayesian linear regression, we introduce a isotropic Gaussian prior over
the weights, with $\alpha$ its inverse variance:
\begin{equation}
  p(\v{w}) = \mathcal{N}(\v{w} | \v{0}, \alpha^{-1} \mbf{I}_d)
\end{equation}
The prior over $\v{w}$ thus induces a prior over \emph{functions} $y(\v{x})$.

The design matrix of basis functions is now $\mPhi \in \mbb{R}^{m \times d}$, so the
hypothesis evaluated on our training examples is an $m$-dim vector
\begin{equation}
  \v{y} = \mPhi \v{w}
\end{equation}
Each element of $\v{y}$ is a linear combination of Gaussians (given by the $\v{w}$
prior), where the weights in the linear combination come from elements of the design
matrix; thus each element of $\v{y}$ is a Gaussian. Then the vector $\v{y}$ itself is a
multivariate Gaussian, completely specified by its mean vector and covariance matrix.
\begin{align}
  \E_{\v{w}}[\v{y}] &= \mPhi\, \E_{\v{w}}[\v{w}] = \v{0} \\
  \trm{cov}[\v{y}]  &= \E_{\v{w}}[\v{y} \v{y}^T]
                      = \mPhi\, \E_{\v{w}}[\v{w} \v{w}^T]\, \mPhi^T
                      = \alpha^{-1} \mPhi\, \mPhi^T
                      \equiv \mbf{K}
\end{align}
where $\mbf{K}$ is the $m \times m$ \tbfit{Gram matrix} with elements
\begin{equation}
  \mbf{K}_{ij} \equiv k(\v{x}_i, \v{x}_j) = \alpha^{-1} \v{\phi}(\v{x}_i)^T\,
  \v{\phi}(\v{x}_j). 
\end{equation}
Our prior over functions is then
\begin{equation}
  p(\v{y}) = \mathcal{N}(\v{y} | \v{0}, \mbf{K})
\end{equation}

\subsection{Basic Gaussian process regression}
For regression, we assume our target variables $t_i$ are given by the hypothesis $\v{y}$
with added Gaussian noise
\begin{equation}
  p(t_i | y_i) = \mathcal{N}(t_i | y_i, \beta^{-1}) \qquad 1 \leq i \leq m
\end{equation}
where $\beta$ is the inverse variance of the Gaussian, and the noise is i.i.d for each
training example. The $m$-dim vector of target variables, $\v{t}$, is then given by a
multivariate Gaussian
\begin{equation}
  p(\v{t} | \v{y}) = \mathcal{N}(\v{t} | \v{y}, \beta^{-1} \mbf{I}_m)
\end{equation}
To get the joint distribution $p(\v{t})$ over all the target variables, we integrate out
$\v{y}$ via \eqref{eq:gauss_bayes_marg}, analogous to how we integrate out the weights
$\v{w}$ in Bayesian regression,
\begin{equation}
  p(\v{t}) = \int p(\v{t} | \v{y}) p(\v{y}) d\v{y} = \mathcal{N}(\v{t} | \v{0}, \mbf{C})
\end{equation}
where $\mbf{C}$ is $m \times m$ with elements
$\mbf{C}_{ij} = \mbf{K}_{ij} + \beta^{-1} \delta_{ij}$.

To make a prediction $t_{m+1}$, based on the $m$ training examples, we form the joint
distribution over $m+1$ examples,
\begin{equation}
  \mathcal{N}(\v{t} | \v{0}, \mbf{C}_{m+1})
\end{equation}
where $\v{t}$ is now $(m+1)$-dim, and the $(m+1) \times (m+1)$ matrix
$\mbf{C}_{m+1}$ is partitioned as
\begin{equation}
  \mbf{C}_{m+1} = 
  \left(
    \begin{matrix}
      \mbf{C}_m & \v{k}\\
      \v{k}^T   & c
    \end{matrix}
  \right),
\end{equation}
where
\begin{itemize}
  \item $\mbf{C}_m$ is $m \times m$ with elements
  $\mbf{C}_{ij} = \mbf{K}_{ij} + \beta^{-1} \delta_{ij}$
  \item $\v{k}$ has elements $k(\v{x}_i, \v{x}_{m+1})$ for $1 \leq i \leq m$
  \item $c = k(\v{x}_{m+1}, \v{x}_{m+1}) + \beta^{-1}$
\end{itemize}

The conditional distribution $p\lp t_{m+1} | \v{t} \rp$ is then given by a Gaussian with
\begin{align}
  m(\v{x}_{m+1})         &= \v{k}^T \mbf{C}_m^{-1}\, \v{t}\\
  \sigma^2 (\v{x}_{m+1}) &= c - \v{k}^T \mbf{C}_m^{-1}\, \v{k}.
\end{align}
which we obtain by using \eqref{eq:gauss_part_cond}. \LATODO{check to make sure this is
  the right eq.}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Neural Networks}
\label{sec:nn}

\subsection{Hyperparameters}
Hypers: learning rate, num iterations, num hidden layers, num units in each layer,
choice of activation function, amount of momentum, minibatch size, regularization type
and amount.

\subsection{Bias/Variance}

Bias/variance is less of a \tit{tradeoff} in neural network training, since we have a
number of methods to reduce both independently.
\begin{itemize}
  \item \tbf{Reduce bias (train set perf):} Bigger network, train longer, (NN arch
  search)
  \item \tbf{Reduce variance (dev set perf):} More data, regularization.. (when changed
  go back and retune bias)
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Variational Methods}
\label{sec:var_meth}
\red{\tbf{Preliminary section!}}

If our prior is not a conjugate prior to our likelihood, we can't compute posterior in
closed form. Try:

\begin{itemize}
  \item approximate intractable posterior $p(\v{\theta} | \v{y})$ with tractable
  $q(\v{\theta} | \gamma)$.
  \item Adjust $\gamma$ to minimize
  $KL \big[p(\v{\theta} | \v{y}) || q(\v{\theta} | \gamma)\big]$ $\to\,$ complicated,
  since we don't know $p(\v{\theta} | \v{y})$. Eventually will use \tbfit{expectation
    propagation} to solve.
  \item Adjust $\gamma$ to minimize
  $KL \big[q(\v{\theta} | \gamma) || p(\v{\theta} | \v{y})\big]$ $\to\,$ simpler:
  \tbfit{variational Bayes}.
  \item \tbf{Variational Bayes}: minimizing
  $KL \big[q(\v{\theta} | \gamma) || p(\v{\theta} | \v{y})\big]$ $\iff$ maximizing ELBO
  $\mathcal{L}$
  \item note the \emph{latent/hidden variables} helping us are the $\v{\theta}$
\end{itemize}



\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{appendices}

\section{Math Stuff}

\subsection{Matrix Stuff}
  
\subsubsection{Matrix Identities}

(NB More useful stuff in \cite{BARBER} chapter 29.)

\paragraph{Trace Identities}
\begin{itemize}
  \item $\trm{Tr}(\mA^T) = \trm{Tr}(\mA)$
  \item $\trm{Tr}(\mA + \mB) = \trm{Tr}(\mA) + \trm{Tr}(\mB)$
  \item $\trm{Tr}(a \mA) = a \trm{Tr}(\mA)$ \qquad (with $a$ a scalar)
  \item $\trm{Tr}(\mA \mB) = \trm{Tr}(\mB \mA)$
  \item $\trm{Tr}(\mA \mB \mC) = \trm{Tr}(\mB \mC \mA) = \trm{Tr}(\mC \mA \mB)$ \qquad
  (cyclicity of trace)
\end{itemize}

\paragraph{Transpose Identities}
\begin{itemize}
  \item $(\mA \mB)^T = \mB^T \mA^T$
\end{itemize}

\paragraph{Determinant Identities}
\begin{itemize}
  \item $|\mA \mB| = |\mA| |\mB|$
  \item $|\mA^{-1}| = \frac{1}{|\mA|}$
  \item $|\mA^T| = |\mA|$
\end{itemize}

\paragraph{Inverse Indentities}
\begin{itemize}
  \item $(\mA \mB)^{-1} = \mB^{-1} \mA^{-1}$
  \item $(\mA^T)^{-1} = (\mA^{-1})^T$
\end{itemize}

\subsubsection{Matrix derivatives}
\begin{equation}
  \frac{\partial}{\partial \v{x}} \left( \v{x}^T \v{a} \right) =
  \frac{\partial}{\partial \v{x}} \left( \v{a}^T \v{x} \right) =
  \v{a}\footnote{\TODO{review understanding of this (I recall this expression was not
      enough to answer my questions on a formula derivation) and give examples.}}  
\end{equation}

For a matrix $\mA(x)$ as a function of a scalar $x$ (equivalently a component $x_i$ of a
vector $\vec x$)

\begin{equation}
  \frac{\partial}{\partial x} \log |\mA| = \trm{Tr} \left(\mA^{-1} \frac{\partial
      \mA}{\partial x} \right)
\end{equation}



\subsubsection{Partitioned Matrix Inversion}
Given block decomposition of matrix into submatrices
$\mA, \mB, \mC, \mD$, the inverse is:
\begin{equation}
  \left(
    \begin{matrix}
      \mA & \mB \\
      \mC & \mD
    \end{matrix}
  \right)^{-1}
  =
  \left(
    \begin{matrix}
      \mM                     & - \mM \mB \mD^{-1} \\
      -\mD^{-1} \mC \mM \quad & \mD^{-1} + \mD^{-1} \mC \mM \mB \mD^{-1}
    \end{matrix}
  \right),
\end{equation}
where
\begin{equation}
  \mM = \left(\mA - \mB \mD^{-1} \mC \right)^{-1}
\end{equation}
(NB compare with Matt Headrick's compendium inversion formula)
  
\subsubsection{Symmetric Matrix Properties}
\TODO{Do this section.}

\subsection{Gaussians}
\subsubsection{Partitioned Gaussians: Conditional and Marginal Distributions}
From \cite[sections 2.3.1 - 2.3.2]{BISHOP}.

Consider a joint Gaussian distribution, $\mathcal{N}(\v{x} | \v{\mu}, \mSigma)$ with
$\mLambda \equiv \mSigma^{-1}$ and
\begin{equation}
  \v{x} =
  \left(
    \begin{matrix}
      \v{x}_a \\
      \v{x}_b
    \end{matrix}
  \right)
  , \quad
  \v{\mu} =
  \left(
    \begin{matrix}
      \v{\mu}_a \\
      \v{\mu}_b
    \end{matrix}
  \right)
\end{equation}

\begin{equation}
  \mSigma =
  \left(
    \begin{matrix}
      \mSigma_{aa} & \mSigma_{ab} \\
      \mSigma_{ba} & \mSigma_{bb}
    \end{matrix}
  \right)
  , \quad
  \mLambda =
  \left(
    \begin{matrix}
      \mLambda_{aa} & \mLambda_{ab} \\
      \mLambda_{ba} & \mLambda_{bb}
    \end{matrix}
  \right).
\end{equation}
(That is, the vector $\v{x}$ is partitioned into $\v{x}_a$ and $\v{x}_b$, which
induces a block partitioning of the covariance matrix $\mSigma$.)

The \tit{conditional distribution} is:
\begin{equation}
  \label{eq:gauss_part_cond}
  p(\v{x}_a | \v{x}_b) = \mathcal{N}(\v{x} | \v{\mu}_{a | b}, \mLambda_{aa}^{-1}),
  \qquad  \trm{where}\quad \v{\mu}_{a | b} \equiv \v{\mu}_a - \mLambda_{aa}^{-1}
  \mLambda_{ab} (\v{x}_b - \v{\mu}_b).
\end{equation}

The \tit{marginal distribution} is:
\begin{equation}
  \label{eq:gauss_part_marg}
  p(\v{x}_a) = \mathcal{N}(\v{x}_a | \v{\mu}_a, \mSigma_{aa})
\end{equation}

\subsubsection{Bayes' theorem for Gaussian variables}
From \cite[section 2.3.3]{BISHOP}. NB This is known as a \tbfit{linear Gaussian model}.

Given a marginal Gaussian distribution for $\v{x}$ and a conditional Gaussian
distribution for $\v{y}$ given $\v{x}$ in the form:
\begin{align}
  p(\v{x})       &= \mathcal{N}(\v{x} | \v{\mu}, \mLambda^{-1})\\
  p(\v{y}|\v{x}) &= \mathcal{N}(\v{y} | \mA \v{x} + \v{b}, \mL^{-1})
\end{align}
we have
\begin{align}
  \label{eq:gauss_bayes_marg}
  p(\v{y})       &= \mathcal{N} (\v{y} |\mA \v{\mu} + \v{b}, \mL^{-1} + \mA
                   \mLambda^{-1} \mA^T )\\
  \label{eq:gauss_bayes_cond}
  p(\v{x}|\v{y}) &= \mathcal{N} ( \v{x} | \mSigma \{ \mA^T \mL (\v{y} - \v{b}) +
                   \mLambda \v{\mu} \}, \mSigma)
\end{align}
where
\begin{equation}
  \mSigma \equiv \left(\mLambda + \mA^T \mL \mA \right)^{-1}.
\end{equation}


\subsection{Matrix Factorization}
\subsubsection{Diagonalization (Eigendecomposition)}
\label{subsubsec:matrix}
A square $N \times N$ symmetric matrix $\mA$ has \emph{real} eigenvalues from its
symmetricity, and $N$ linearly independent eigenvectors.  We can write
$\mA = \mU \mD \mU^{-1} = \mU \mD \mU^T$, with $\mU$ an orthogonal\footnote{An
  \emph{orthogonal} matrix should really be called an \emph{orthonormal} matrix, as its
  columns are not just linearly independent, but also normalized.} matrix with columns
$\v{u}_i$ equal to $\mA$'s eigenvectors, and $\mD$ a diagonal matrix of $\mA$'s
eigenvalues, $\lambda_i$.

We can also write down the \tbfit{spectral decomposition} of $\mA$:
\begin{equation}
  \mA = \mU \mD \mU^T = \sum_{i=1}^N \lambda_i\, \v{u}_i\, \v{u}_i^T.
\end{equation}


\subsubsection{Singular value decomposition}
\label{subsubsec:SVD}
Using conventions of \cite{PRESS}, section 2.6.

An $M \times N$ matrix $\mA$ can be written as $\mU \mD \mV^T$, where $\mU$ is
$M \times N$, $\mD$ is $N \times N$ and diagonal, and $\mV$ is $N \times N$ and
orthogonal.

If $\mA$ is square, the columns of $\mV$ are its eigenvectors, and $\mD$ are its
eigenvalues.

\tbf{Uses}:
\begin{itemize}
  \item If $M \geq N$, cols of $\mU$ are an orthonormal basis for space spanned by cols
  of $\mA$
\end{itemize}

\TODOFIN{}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Linear Regression Hierarchy}
Linear regression starting from maximum likelihood, through variational Bayes.
\subsection{Maximum likelihood}
Maximize likelihood $p(y | \theta ; \beta)$ wrt $\theta$, $\beta$.

\subsection{Maximum a posteriori}
Introduce prior $p(\theta ; \alpha) = \exp(-\alpha \theta^T \theta)$. Find $\theta$
by maximizing $p(y | \theta ; \beta) p(\theta ; \alpha)$ wrt $\theta$.

\begin{itemize}
  \item Q: what is proper way to estimate $\beta$ and $\alpha$ here?
  \item half-A: not sure if you can? You typically use the evidence approx (see ``Full
  Bayesian'', below) to find best values for $\beta$ and $\alpha$.. OTOH we found most
  likely $\beta$ in previous section (``Maximum Likelihood''), so why can't we do it
  here?  \LATODO{finish answering this.}
\end{itemize}

\subsection{Full Bayesian (stationary prior)}
Calculate posterior
$p(\theta | y) = p(y | \theta ; \beta) p(\theta ; \alpha) / p(y ; \alpha, \beta)$, where
$p(y ; \alpha, \beta) = \int_\theta p(y | \theta ; \beta) p(\theta ; \alpha) d\theta$ is
the \tbfit{evidence}. Determine values of hypers $\alpha, \beta$ by maximizing evidence
wrt them. This is difficult to do directly, as derivs wrt $\alpha, \beta$ are hard to
compute, so use EM algorithmm.

\subsection{Variational Bayes (non-stationary prior)}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\red{TODOs}}

\begin{itemize}
  \item Conjugate priors
  \item EM algorithm and variational inference
  \item Kernel theory, dual representation, kernel trick, \emph{etc}..
  \item MCMC methods
  \item SVMs
  \item Boosting, bagging, random forests, regression trees, \emph{etc}..
\end{itemize}


\end{appendices}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\printbibliography[heading=bibintoc]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-engine: xetex
%%% End:
