%% Machine Learning cheat sheet
%% (compile with XeTeX. see notes in structure_cheatSheet.tex if no XeTeX.)

\documentclass[11pt]{article}
\synctex=1
\input{structure_cheatSheet.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{ML Cheatsheet}
\author{Steve Young}
\abstract{Everything I know about machine learning.}

\begin{document}
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conventions}
\label{sec:conv}
\paragraph{Math Notation}
\begin{itemize}
  \item $x \in \{0, 1\}^r$ : $x$ is a vector of form \eeg $(0, 1, 1, 0, \dots, 1, 0)$ of
  length $r$.
  \item $\sim$ : random variable drawn from a distribution\\
  $\propto$ : ``proportional to''
  \item $\mbf{1}$($\cdot$) : indicator function --- 1 when arg is true, 0 when arg is
  false.
  \item $\mbf{I}_k$: $k \times k$ identity matrix
  \item Boldface capital letters are matrices, \eeg $\mA = \mU \mW \mV^T$
  \item $\log$ is base $e$ by default $\to$ entropy in nats.
\end{itemize}

\paragraph{Ng CS229 / DeepLearning.ai}
\begin{itemize}
  \item $m$ : number of training examples in the dataset
  \item $n$ : dimension of training examples
  \item $x^{(i)} \in \mbb{R}^{n}$ : $i^{th}$ training example (\emph{column} vector),
  $1 \leq i \leq m$
  \item $y^{(i)}$ : $i^{th}$ output (\emph{column} vector), $1 \leq i \leq m$
  \item $x_j$ : $j^{th}$ component of a training example, $1 \leq j \leq n$.
  \item $\mX \in \mbb{R}^{n \times\, m}$ : (input/design) matrix (training examples are
  \emph{column} vectors)\footnote{Note our non-conventional definition of the design
    matrix $\mX$. The more conventional version is denoted $\mbbb{X}$.}
  \item $\mbbb{X} \in \mbb{R}^{m \times\, n}$ : (input/design) matrix (training examples
  are \emph{row} vectors)
\end{itemize}



\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Linear Regression}
\label{sec:linreg}
\subsection{Basics}
\begin{itemize}
  \item Hypothesis
  $h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots = \sum_{j=0}^{n}
  \theta_j x_j \equiv \theta^T x$, where $x_0 = 1$.
  \item Cost function
  \begin{equation}
    J(\theta) = \frac{1}{2} \sum_{i=1}^m \left(h_\theta(x^{(i)}) - y^{(i)}\right)^2
  \end{equation}
  \item Analytically solve via normal equations, where $\mX$ is the augmented
  $(n+1) \times m$ design matrix
  \begin{equation}
    \vec{\theta} = \left(\mX \mX^T \right)^{-1} \mX\, \vec{y}
  \end{equation}
  where $\vec{\theta}$ and $\vec{y}$ are column vectors of the $n+1$ weights and $m$
  outputs, respectively.
\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Logistic Regression}
\label{sec:logreg}
\subsection{2 classes}
With 2 labels $y \in \{0,1\}$ and $m$ training examples $x^{(i)},\ \ 1 \leq i \leq m$,
we have
\begin{equation}
  h_\theta (x) = \frac{1}{1 + e^{-\theta^T x}}
\end{equation}
\begin{equation}
  p(y|x;\theta) = (h_\theta (x))^y (1 - h_\theta (x))^{1-y}
\end{equation}
\begin{equation}
  L(\theta) = \prod_{i=1}^m (h_\theta (x^{(i)}))^{y^{(i)}} (1 - h_\theta
  (x^{(i)}))^{1-y^{(i)}} 
\end{equation}
\begin{equation}
  l(\theta) = \log L(\theta) = \sum_{i=1}^m y^{(i)} \log h_\theta (x^{(i)}) + (1 -
  y^{(i)}) \log(1 - h_\theta  (x^{(i)}))
\end{equation}
with deriv
\begin{equation}
  \frac{\partial}{\partial \theta_j} l(\theta) = x_j^T (y - h_\theta (x))
\end{equation}
We can't minimize the log-likelihood analytically, so we must use numerical
optimization.

The log-likelihood is just the cross entropy
\begin{equation}
  H(p,q) = -\sum_i p_i \log q_i
\end{equation}
with $p$ the actual outputs, and $q$ the hypothesis $h_\theta(x)$

\subsection{2+ classes}

\tbf{One-versus-all:} For $k$ classes, do $k$ normal log regs. Each has two classes: the
target class, and all the rest. New examples are classified by whichever of these $k$
log regs. has highest score.

\tbf{Softmax regression:} Model classification cases as \emph{multinomial}
distribution. For $k$ classes, hypothesis is $k$-dim vector
\begin{equation}
  h_\theta (x) = \frac{1}{\sum_{l=1}^k \exp(\theta^{(l)T} x)} \times
  \big[\exp(\theta^{(1)T} x), \dots, \exp(\theta^{(k)T} x)\big] 
\end{equation}
so that the final output layer has $k$ units. The log-likelihood is
\begin{equation}
  l(\theta) = - \sum_{i=1}^m \sum_{l=1}^k \mbf{1}(y^{(i)} = l)
  \log \frac{\exp(\theta^{(l)T} x)}{\sum_{m=1}^k \exp(\theta^{(m)T} x)} 
\end{equation}

\TODO{show how 2-class log reg cost func can be written this way. write down deriv of
  cost func. clean up notation.}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Gaussian Discriminant Analysis (GDA) and Naive Bayes}
\label{sec:GDA_nbayes}
\tbfit{Generative algorithm}: Instead of modeling $p(y|x)$, model $p(y)$ and $p(x|y)$,
then get posterior $p(y|x)$ via Bayes' theorem.

\subsection{GDA}
Can use GDA for classification problem when input features are continuous-val random
vars. Models $p(x|y)$ as multivariate Gaussian. Model is
\begin{align}
  y     &\sim \trm{Bernoulli}(\phi) \nncr
  x|y=0 &\sim \mathcal{N(\mu_0, \mSigma)} \nncr
  x|y=1 &\sim \mathcal{N(\mu_1, \mSigma)}
\end{align}

\TODOFIN{}

\subsection{Naive Bayes (for text classification)}
For training examples $x \in \{0,1\}^n$ (a \tbfit{vocabulary}\footnote{the $j^{th}$
  entry is 1 if the example contains the $j^{th}$ word in the vocabulary. \tbf{Example}:
  If the vocab is \{cats, rats, bats\}, then $n=3$, and a training example that contains
  ``cats'', ``rats'', but not ``bats'', would be $x = (1, 1, 0)$.} of length $n$),
assume the components of an example, $x_j$, are conditionally independent given $y$
(\tbfit{Naive Bayes assumption}).
\begin{equation}
  p(x_1, \dots, x_n | y) = \prod_{i=1}^n p(x_i | y)
\end{equation}

Model is parameterized by
\begin{equation}
  \phi_y = p(y=1),\qquad \phi_{j|y=1} = p(x_j=1|y=1), \qquad \phi_{j|y=0} = p(x_j=1|y=0)
\end{equation}
where $j \in (1, n)$, for a total of $2n +1$ parameters. Likelihood is
\begin{equation}
  L(\phi_y, \{\phi_{j|y=1}, \phi_{j|y=0}\}_{j=1}^n) = \prod_{i=1}^m p(x^{(i)}, y^{(i)})
\end{equation}
which has MLE values
\begin{align}
  \phi_{j|y=1} &= \trm{fraction of spam (y=1) in which word j appears}\nncr
  \phi_{j|y=0} &= \trm{fraction of non-spam (y=0) in which word j appears}\nncr
  \phi_{y}     &= \trm{ fraction of training examples that are spam}
\end{align}
To make predictions, we don't need the evidence $p(x)$; we just need to compare
\begin{align}
  p(y=1|x) &\propto p(x|y=1)p(y=1)\nncr
  p(y=0|x) &\propto p(x|y=0)p(y=0)
\end{align}
and pick the class that has the higher value (un-normalized posterior).

\TODOFIN{show how last two eqs are actually calculated}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bias/Variance Tradeoff}
\label{sec:bias_var}
\begin{itemize}
  \item \tbf{High bias}: underfitting. high training error and test error.
  \item \tbf{High variance}: overfitting. \tit{low} training error and
  \tit{high} test error
\end{itemize}

Can't use the test set to decide on values of hypers (\eeg number of parameters in
model) because ``we could just tweak the values of the hypers until the estimator
performs optimally [on the test set]''.

Cross-validation allows you to do model scoring (on frequentist stats, versus \eeg info
criteria for Bayesian stats.. \TODO{verify i know what i'm talking about}).

Can average together multiple models with high capacity (low bias, but high
variance). The result of combining is to \emph{reduce} the variance of the combined
model.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Model Assessment}
\label{sec:model_ass}
\subsection{Classification}
\subsubsection{Precision/Recall}
For 2 classes, with \{TN, FN, FP, TP\} = \{true negatives, false negatives, false
positives, true positives\}, we have the \tbfit{loss} or \tbfit{confusion matrix}:
\begin{equation}
  \left(
    \begin{matrix}
      \trm{TN} & \trm{FP}\\
      \trm{FN} & \trm{TP}
    \end{matrix}
  \right),
\end{equation}
where the rows/columns are actual/predicted numbers of examples not-in-class (negative),
or in-class (positive). We define the \tbfit{precision} and \tbfit{recall}:
\begin{equation}
  \trm{precision} = \frac{\trm{TP}}{\trm{TP} + \trm{FP}}, \qquad
  \trm{recall} = \frac{\trm{TP}}{\trm{TP} + \trm{FN}}.
\end{equation}
\tbfit{Precision} is ability of classifier to not classify actual negatives as positive
(\eie \emph{low} false positives). So \eeg if positive is ``person doesn't have
cancer'', we want \emph{high precision}.

\tbfit{Recall} is ability of classifer to find all positive samples.

\subsubsection{Receiver operating characteristic curve (ROC)}
\tbfit{True Positive Rate (TPR):} same as recall \\
\tbfit{True Negative Rate (TNR):} fraction of actual negatives that are classified as
negative $\trm{TN}/(\trm{TN} + \trm{FP})$
\tbfit{False Positive Rate (FPR):} 1 - TNR

ROC plots TPR versus FPR.

\TODO{More on why the ROC is useful. Maybe a picture.}

\subsubsection{F1 Score}
The F1 score is the \emph{harmonic mean} of precision and recall. It's an ok one-number
metric for evaluating effectiveness of different classifiers on problem:
\begin{equation}
  \label{eq:F1}
  \textrm{F1} = \frac{2}{\left(\frac{1}{\textrm{prec}} + \frac{1}{\textrm{rec}}\right)}
\end{equation}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{K-means}
\label{sec:kmeans}
\begin{itemize}
  \item Setup: Select any $K$ points from data to serve as initial centroids of the
  clusters.
  \item Repeat until (stopping criteria: \eeg no points change clusters, sum of
  distances is minimized, some total number of iterations..):
  \begin{enumerate}
    \item Assign each datapoint to the cluster with closest centroid.
    \item Compute $K$ new centroids, each the mean of the datapoints in its cluster.
  \end{enumerate}
\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{PCA}
\label{sec:pca}
PCA is unsupervised technique for finding directions of most variance in dataset. The
\tbfit{principal components} are a set of $n$ orthonormal $n$-dim basis vects along the
directions of maximum variance, ordered by decreasing variance in their directions.

\tbf{To calc:}
\begin{itemize}
  \item \tbf{via eigenvectors:} Principal components of \emph{de-meaned} $n \times m$
  design matrix $\mX$ are the eigenvectors of the covariance matrix of $\mX$,
  $\mbf{C_X} = \frac{1}{m} \mX \mX^T$.
  \item \tbf{via SVD:} Define $\mY = \frac{1}{\sqrt{m}} \mX^T$, and take its SVD
  \ref{subsubsec:SVD}, which is $\mY = \mU \mW \mV^T$. Then $\mU^T$ is $n \times m$
  matrix that projects $\mX$ onto its principal components, $\mW$ is the matrix of
  principal comp values, and $\mV$ is orthonormal matrix of the principal
  components. \TODO{Verify/fix statement on $\mU$ projection.}
\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Time Series}
\label{sec:timeseries}
\subsection{Stationarity}

\begin{definition}[Second Order Stationarity] is when correlation between sequential
  observations is only a function of the lag.
\end{definition}

\begin{itemize}
  \item \tbf{Dickey-Fuller test:} tests for stationarity of AR model. Null hypothesis is
  ``series is \tit{non-stationary}''.
\end{itemize}

\subsection{ARCH}
\begin{itemize}
  \item{\tbf{Box-Jenkins:} systematic methodology for identifying and estimating models
    that can incorporate both AR and MA.}
\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Gaussian Processes}
\label{sec:gp}
\subsection{Prior over functions, from basis functions}
Following the treatment in section 6.4 of BISHOP (with different notation than above).

Let $d$ be the number of \emph{basis functions} $\vec \phi$, which are non-linear
functions of the $n$ dimensional training examples $\vec x$. We write the hypothesis as
\begin{equation}
  y(\vec x) = \vec w^T \vec \phi(\vec x)
\end{equation}
where $\vec w$ is a $d$ dimensional vector of weights.

As in MAP and Bayesian linear regression, we introduce a isotropic Gaussian prior over
the weights, with $\alpha$ its inverse variance:
\begin{equation}
  p(\vec w) = \mathcal{N}(\vec w | \vec 0, \alpha^{-1} \mbf{I}_d)
\end{equation}
The prior over $\vec w$ thus induces a prior over \emph{functions} $y(\vec x)$.

The design matrix is now $\mbbb{X} \in \mbb{R}^{m \times\, d}$, so the hypothesis
evaluated on our training examples is an $m$ dim vector
\begin{equation}
  \vec y = \mbbb{X} \vec w
\end{equation}
Each element of $\vec y$ is linear combination of Gaussians (given by the $\vec w$
prior), where the weights in the linear combination come from elements of the design
matrix. Thus the vector $\vec y$ itself is a multivariate Gaussian, completely specified
by its mean vector and covariance matrix.
\begin{align}
  \mathbb{E}[\vec y]   &= \mbbb{X}\, \mathbb{E}[\vec w] = \vec 0 \\
  \textrm{cov}[\vec y] &= \mathbb{E}[\vec y \vec y^T]
                         = \mbbb{X}\, \mathbb{E}[\vec w \vec w^T]\, \mbbb{X}^T
                         = \alpha^{-1} \mbbb{X}\, \mbbb{X}^T
                         \equiv \mbf{K}
\end{align}
where $\mbf{K}$ is the $m \times m$ \emph{Gram matrix} with elements
\begin{equation}
  \mbf{K}_{ij} \equiv k(\vec x_i, \vec x_j) = \alpha^{-1} \vec \phi(\vec x_i)^T\, \vec
  \phi(\vec x_j).
\end{equation}
Our prior over functions is then
\begin{equation}
  p(\vec y) = \mathcal{N}(\vec y | \vec 0, \mbf{K})
\end{equation}

\subsection{Basic Gaussian process regression}
For regression, we assume our target variables are given by the hypothesis $\vec y$ with
added Gaussian noise
\begin{equation}
  p(t_i | y_i) = \mathcal{N}(t_i | y_i, \beta^{-1}) \qquad 1 \leq i \leq m
\end{equation}
where $\beta$ is the inverse variance of the Gaussian, and the noise is i.i.d for each
training example. The $m$ dim vector of target variables is then given by a multivariate
Gaussian
\begin{equation}
  p(\vec t | \vec y) = \mathcal{N}(\vec t | \vec y, \beta^{-1} \mbf{I}_m)
\end{equation}
To get the joint distribution $p(\vec t)$ over all the target variables, we integrate
out $\vec y$ via \ref{eq:gauss_bayes_marg}, analagous to how we integrate out the
weights $\vec w$ in Bayesian regression,
\begin{equation}
  p(\vec t) = \int p(\vec t | \vec y) p(\vec y) d\vec y =
  \mathcal{N}(\vec t | \vec 0, \mbf{C})
\end{equation}
where $\mbf{C}$ is $m \times m$ with elements
$\mbf{C}_{ij} = \mbf{K}_{ij} + \beta^{-1} \delta_{ij}$.

To make a prediction based on the $m$ training examples, we form the joint distribution
over $m+1$ examples,
\begin{equation}
  \mathcal{N}(\vec t_{m+1} | \vec 0, \mbf{C}_{m+1})
\end{equation}
with the $(m+1) \times (m+1)$ matrix $\mbf{C}_{m+1}$ partitioned as
\begin{equation}
  \mbf{C}_{m+1} = 
  \left(
    \begin{matrix}
      \mbf{C}_m & \vec k\\
      \vec k^T  & c
    \end{matrix}
  \right),
\end{equation}
where
\begin{itemize}
  \item $\mbf{C}_m$ is $m \times m$ with elements $\mbf{C}_{ij} = \mbf{K}_{ij} +
  \beta^{-1} \delta_{ij}$
  \item $\vec k$ has elements $k(\vec x_i, \vec x_{m+1})$ for $1 \leq i \leq m$
  \item $c = k(\vec x_{m+1}, \vec x_{m+1}) + \beta^{-1}$
\end{itemize}

The conditional distribution $p(t_{m+1} | \vec t)$ is then given by a Gaussian with
\begin{align}
  m(\vec x_{m+1}) &= \vec k^T \mbf{C}_m^{-1}\, \vec t\\
  \sigma^2 (\vec x_{m+1}) &= c - \vec k^T \mbf{C}_m^{-1}\, \vec k.
\end{align}
which we obtain by using \ref{eq:gauss_part_cond}.

\TODO{check this to make sure it's the right eq.}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Neural Networks}
\label{sec:nn}

\subsection{Hyperparameters}
Hypers: learning rate, num iteractions, num hidden layers, num units in each layer,
choice of activation function, amout of momentum, minibatch size, regularization type
and amount.

\subsection{Bias/Variance}

Bias/variance is less of a \tit{tradeoff} in neural network training, since we have a
number of methods to reduce both independently.
\begin{itemize}
  \item \tbf{Reduce bias (train set perf):} Bigger network, train longer, (NN arch
  search)
  \item \tbf{Reduce variance (dev set perf):} More data, regularization.. (when changed
  go back and retune bias)
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Variational Methods}
\label{sec:var_meth}
\blue{\tbf{Preliminary section!}}

If our prior is not a conjugate prior to our likelihood, we can't compute posterior in
closed form. Try:

\begin{itemize}
  \item approximate intractable posterior $p(\vec{\theta} | \vec{y})$ with tractable
  $q(\vec{\theta} | \gamma)$.
  \item Adjust $\gamma$ to minimize
  $KL \big[p(\vec{\theta} | \vec{y}) || q(\vec{\theta} | \gamma)\big]$ $\to\,$
  complicated, since we don't know $p(\vec{\theta} | \vec{y})$. Eventually will use
  \tbfit{Expectation Propagation} to solve.
  \item Adjust $\gamma$ to minimize
  $KL \big[q(\vec{\theta} | \gamma) || p(\vec{\theta} | \vec{y})\big]$ $\to\,$ simpler:
  \tbfit{Variational Bayes}.
  \item \tbf{Variational Bayes}: minimizing
  $KL \big[q(\vec{\theta} | \gamma) || p(\vec{\theta} | \vec{y})\big]$ $\iff$ maximizing
  ELBO $\mathcal{L}$
  \item note the \emph{latent/hidden variables} helping us are the $\vec{\theta}$
\end{itemize}



\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{appendices}
  \section{Math Stuff}

  \subsection{Matrix Stuff}
  \subsubsection{Partitioned Matrix Inversion}
  Given block decomposition of matrix into submatrices
  $\mA, \mB, \mC, \mD$, the inverse is:
  \begin{equation}
    \left(
      \begin{matrix}
        \mA & \mB \\
        \mC & \mD
      \end{matrix}
    \right)^{-1}
    =
    \left(
      \begin{matrix}
        \mM & - \mM \mB \mD^{-1} \\
        - \mD^{-1} \mC \mM \quad & \mD^{-1} + \mD^{-1} \mC \mM \mB \mD^{-1}
      \end{matrix}
    \right),
  \end{equation}
  where
  \begin{equation}
    \mM = \left(\mA - \mB \mD^{-1} \mC \right)^{-1}
  \end{equation}
  (NB compare with Matt Headrick's compendium inversion formula)


\subsection{Gaussians}
\subsubsection{Partitioned Gaussians: Conditional and Marginal Distributions}
From BISHOP, sections 2.3.1 - 2.3.2.

Consider a joint Gaussian distribution, $\mathcal{N}(\vec{x} | \vec{\mu}, \mSigma)$ with
$\mLambda \equiv \mSigma^{-1}$ and
\begin{equation}
  \vec{x} =
  \left(
    \begin{matrix}
      \vec{x}_a \\
      \vec{x}_b
    \end{matrix}
  \right)
  , \quad
  \vec{\mu} =
  \left(
    \begin{matrix}
      \vec{\mu}_a \\
      \vec{\mu}_b
    \end{matrix}
  \right)
\end{equation}

\begin{equation}
  \mSigma =
  \left(
    \begin{matrix}
      \mSigma_{aa} & \mSigma_{ab} \\
      \mSigma_{ba} & \mSigma_{bb}
    \end{matrix}
  \right)
  , \quad
  \mLambda =
  \left(
    \begin{matrix}
      \mLambda_{aa} & \mLambda_{ab} \\
      \mLambda_{ba} & \mLambda_{bb}
    \end{matrix}
  \right).
\end{equation}
(That is, the vector $\vec{x}$ is partitioned into $\vec{x}_a$ and $\vec{x}_b$, which
induces a block partitioning of the covariance matrix $\mSigma$.)

The \tit{conditional distribution} is:
\begin{align}
  p(\vec{x}_a | \vec{x}_b) &= \mathcal{N}(\vec{x} | \vec{\mu}_{a | b},
                              \mLambda_{aa}^{-1}) \\ 
  \vec{\mu}_{a | b}        &= \vec{\mu}_a - \mLambda_{aa}^{-1} \mLambda_{ab}
                              (\vec{x}_b - \vec{\mu}_b)   \label{eq:gauss_part_cond} 
\end{align}

The \tit{marginal distribution} is:
\begin{equation}
  p(\vec{x}_a) = \mathcal{N}(\vec{x}_a | \vec{\mu}_a,
  \mSigma_{aa}) \label{eq:gauss_part_marg} 
\end{equation}

\subsubsection{Bayes' theorem for Gaussian variables}
From BISHOP, section 2.3.3. NB This is known as a \tit{linear Gaussian model}.

Given a marginal Gaussian distribution for $\vec{x}$ and a conditional Gaussian
distribution for $\vec{y}$ given $\vec{x}$ in the form:
\begin{align}
  p(\vec{x})         &= \mathcal{N}(\vec{x} | \vec{\mu}, \mLambda^{-1}) \\
  p(\vec{y}|\vec{x}) &= \mathcal{N}(\vec{y} | \mA \vec{x} + \vec{b}, \mL^{-1}),
\end{align}
we have
\begin{align}
  p(\vec{y})         &= \mathcal{N} (\vec{y} |\mA \vec{\mu} + \vec{b}, \mL^{-1}
                        + \mA \mLambda^{-1} \mA^T )   \label{eq:gauss_bayes_marg} \\
  p(\vec{x}|\vec{y}) &= \mathcal{N} ( \vec{x} | \mSigma \{ \mA^T \mL
                        (\vec{y} - \vec{b}) + \mLambda \vec{\mu} \},
                       \mSigma)   \label{eq:gauss_bayes_cond} 
\end{align}
where
\begin{equation}
  \mSigma \equiv \left(\mLambda + \mA^T \mL \mA \right)^{-1}
\end{equation}


\subsection{Matrix Factorization}
\subsubsection{Diagonalization (Eigendecomposition)}
\label{subsubsec:matrix}
A square $N \times N$ symmetric matrix $\mA$ has real eigenvalues from its symmetricity,
and $N$ linearly independent eigenvectors. We can write
$\mA = \mO \mD \mO^{-1} = \mO \mD \mO^T$, with $\mO$ an orthogonal\footnote{An
  \emph{orthogonal} matrix should really be called an \emph{orthonormal} matrix, as its
  columns are linearly indepdendent, but also normalized.} matrix with columns equal to
$\mA$'s eigenvectors, and $\mD$ a diagonal matrix of $\mA$'s eigenvalues.


\subsubsection{Singular value decomposition}
\label{subsubsec:SVD}
An $M \times N$ matrix $\mA$ can be written as $\mU \mD \mV^T$, where $\mU$ is
$M \times N$, $\mD$ is $N \times N$ and diagonal, and $\mV$ is $N \times N$ and
orthogonal.

If $\mA$ is square, the columns of $\mV$ are its eigenvectors, and $\mD$ are its
eigenvalues.

\tbf{Uses}:
\begin{itemize}
  \item If $M \geq N$, cols of $\mU$ are an orthonormal basis for space spanned by cols
  of $\mA$
\end{itemize}

\TODOFIN{}

\subsection{Matrix derivatives}

\begin{equation}
  \frac{\partial}{\partial \vec{x}} \left( \vec{x}^T \vec{a} \right) =
  \frac{\partial}{\partial \vec{x}} \left( \vec{a}^T \vec{x} \right) =
  \vec{a} 
\end{equation}

\TODO{review understanding of this (I recall this expression was not enough to answer my
  questions on a formula derivation) and give examples.}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Linear Regression Hierarchy}
Linear regression starting from maximum likelihood, through variational Bayes.
\subsection{Maximum likelihood}
Maximize likelihood $p(y | \theta ; \beta)$ wrt $\theta$, $\beta$.

\subsection{Maximum a posteriori}
Introduce prior $p(\theta ; \alpha) = \exp(-\alpha \theta^T \theta)$. Find $\theta$
by maximizing $p(y | \theta ; \beta) p(\theta ; \alpha)$ wrt $\theta$.
\\
Q: what is proper way to estimate $\beta$ and $\alpha$ here?

\subsection{Full Bayesian (stationary prior)}
Calc posterior
$p(\theta | y) = p(y | \theta ; \beta) p(\theta ; \alpha) / p(y ; \alpha, \beta)$, where
$p(y ; \alpha, \beta) = \int_\theta p(y | \theta ; \beta) p(\theta ; \alpha)$ is the
evidence. Determine values of hypers $\alpha, \beta$ by maximizing evidence. This is
difficult to do directly, as derivs wrt $\alpha, \beta$ are hard to compute, so use EM
algorithm.

\subsection{Variational Bayes (non-stationary prior)}

\end{appendices}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-engine: xetex
%%% End:
