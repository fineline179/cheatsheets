%% Prob/Stats cheat sheet
%% (compile with XeTeX. see notes in structure_cheatSheet.tex if no XeTeX.)

\documentclass[11pt]{article}
\synctex=1
\input{structure_cheatSheet.tex}
\usepackage{xfrac}
\addbibresource{refs_cheatSheet.bib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Prob/Stats Cheatsheet}
\author{Steve Young}
\abstract{Everything I know about prob/stats/maybe information theory too..}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conventions}
\paragraph{Math Notation}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Distributions}
\subsection{Gaussians}
\subsubsection{Basics}
\begin{enumerate}
  \item To start with, \emph{memorize} that
  \begin{equation}
    \boxed{\int_{-\infty}^{\infty} dx\, e^{-x^2} = \pi^{1/2}}
  \end{equation}

  \item Next, anything multiplying the $x^2$ in the integrand is present in inverse
  under the square root.
  \begin{equation}
    \int_{-\infty}^{\infty} dx\, e^{-\trm{stuff}\, x^2} =
    \lp \frac{\pi}{\trm{stuff}} \rp^{1/2}
  \end{equation}
  so, for example:
  \begin{equation}
    \label{eq:int_gauss_a2}
    \int_{-\infty}^{\infty} dx\, e^{-\frac{1}{2} a x^2} = \lp \frac{2 \pi}{a} \rp^{1/2} 
  \end{equation}

  \item The traditional Gaussian pdf has $a = 1 / \sigma^2$, and is easily seen to be
  \begin{equation}
    \mathcal{N}(x|0,\sigma^2) = \frac{1}{\lp 2 \pi \sigma^2 \rp^{1/2}}\,
    e^{-\frac{1}{2 \sigma^2} x^2}
  \end{equation}
\end{enumerate}

\subsubsection{Differentiation moment trick}
\label{subsubsec:dmt}
By differentiating Eq. \ref{eq:int_gauss_a2} wrt $a$, we obtain an expression for
integrals of the form $\int_{-\infty}^{\infty}dx \, x^{2n} e^{-\frac{1}{2} a x^2}$, with
$n \in \mathbb{Z^+}$.

\eeg for $n=1$:
\begin{equation}
  - 2 \frac{d}{da} \int_{-\infty}^{\infty}dx \, e^{-\frac{1}{2} a x^2} =
  \int_{-\infty}^{\infty}dx \, x^2 e^{-\frac{1}{2} a x^2} =
  - 2 \frac{d}{da} \lp \frac{2\pi}{a} \rp^{1/2} =
  \lp \frac{2\pi}{a} \rp^{1/2} \, \frac{1}{a} 
\end{equation}
For $n=2$:
\begin{equation}
  \lp -2 \frac{d}{da} \rp^2 \int_{-\infty}^{\infty}dx \, e^{-\frac{1}{2} a x^2} =
  \int_{-\infty}^{\infty}dx \, x^4 e^{-\frac{1}{2} a x^2} =
  \lp -2 \frac{d}{da} \rp^2 \lp \frac{2\pi}{a} \rp^{1/2} =
  \lp \frac{2\pi}{a} \rp^{1/2} \, \frac{1}{a} \, \frac{3}{a}
\end{equation}
Generally:
\begin{equation}
  \int_{-\infty}^{\infty}dx \, x^{2n} e^{-\frac{1}{2} a x^2} =
  \lp \frac{2\pi}{a} \rp^{1/2} \frac{1}{a^n} (2n-1) (2n-3) \cdot\cdot\cdot 5 \cdot 3
  \cdot 1 
\end{equation}

We thus obtain an expression for the expectation value of $x^{2n}$ under the Gaussian
distribution: 
\begin{equation}
  \label{eq:x2n_moments}
  \langle x^{2n} \rangle =
  \frac{\int_{-\infty}^{\infty}dx \, x^{2n} e^{-\frac{1}{2} a x^2}}
       {\int_{-\infty}^{\infty}dx \, e^{-\frac{1}{2} a x^2}} =
  \frac{1}{a^n} (2n-1) (2n-3) \cdot\cdot\cdot 5 \cdot 3 \cdot 1
\end{equation}

\subsubsection{Gaussian with Linear Term}
To evaluate integrals of the form
\begin{equation}
  \label{eq:int_gauss_a2_j}
  \int_{-\infty}^{\infty} dx\, e^{-\frac{1}{2} a x^2 + J x},
\end{equation}
first complete the square in the exponent
\begin{equation}
  -\frac{a}{2} x^2 + J x = -\frac{a}{2} \lp x^2 - \frac{2Jx}{a} \rp =
  -\frac{a}{2} \lp x - \frac{J}{a} \rp^2 + \frac{J^2}{2a}
\end{equation}
which gives
\begin{equation}
  \int_{-\infty}^{\infty} dx\, e^{-\frac{1}{2} a x^2 + J x} =
  \int_{-\infty}^{\infty} dx\, e^{-\frac{1}{2} a\, \lp x - J/a \rp} e^{J^2 / 2a} =
  \lp\frac{2\pi}{a} \rp^{1/2} \, e^{J^2 / 2a}
\end{equation}
where the integral is done by shifting $x \to x + J a$ (or noting that the infinite
integral of a Gaussian is independent of its mean.)

By differentiating this expression wrt $J$ repeatedly, and finally setting $J=0$, we
obtain another way of deriving the moments of the Gaussian,
Eq. \eqref{eq:x2n_moments}. This motivates the introduction of the \tbfit{moment
  generating function}: given a pdf $p(x)$, the moment generating function is
\begin{equation}
  \psi_x(J) = \E_x \lb e^{J x} \rb = \int_{-\infty}^{\infty} dx\, e^{J x} p(x)
\end{equation}
which satisfies
\begin{equation}
  \la x^n \ra = \frac{d^n\, \psi_x(J)}{d J^n}\Bigg|_{J=0}
\end{equation}

% so that the moment generating function for the Gaussian distribution is
% \begin{equation}
%   \frac{1}{\lp 2 \pi \sigma^2 \rp^{1/2}}\, \int_{-\infty}^{\infty} dx\,
%   e^{J x}  e^{-\frac{1}{2 \sigma^2} x^2}, 
% \end{equation}



\TODO{Finish. Figure out clear way to include normalization factor of pdf in exposition}

\subsubsection{Multivariate Gaussians}
Promoting $a$ to a real $N \times N$ symmetric matrix $\mA$, and x and J to a $N$-dim
vectors $\v{x}$ and $\v{J}$ with components $x_i$ and $J_i$, we have the multivariate
Gaussian integral 
\begin{equation}
  \prod_{i=1}^N \lp \int_{-\infty}^{\infty} dx_i \rp
  e^{-\frac{1}{2} \v{x}^T \mA \v{x} + \v{J}^T \v{x}} =
  \lp \frac{(2 \pi)^{N/2}}{|\mA|^{1/2}} \rp e^{\frac{1}{2} \v{J}^T \mA^{-1} \v{J}}
\end{equation}
\TODOFIN{}

A detailed workthrough of multivariate Gaussian integrals is in
\href{http://vixra.org/abs/1404.0026}{\ttt{viXra:1404.0026}}.

\subsection{Bernoulli}
For $x \in \{0, 1\}$, Bernoulli dist parameterized by $\mu$, with
\begin{equation}
  p(x; \mu) = \mu^x (1-\mu)^{1-x}
\end{equation}

\subsection{Exponential Family}
These are pdfs of the form
\begin{equation}
  p(x;\theta) = h(x) \exp \lb \theta^T T(x) - A(\theta) \rb
\end{equation}
where
\begin{itemize}
  \item $\theta$ is the \tbfit{natural} or \tbfit{canonical parameter}
  \item $T(x)$ is the \tbfit{sufficient statistic}
  \item $A(\theta)$ is the \tbfit{log partition function}
  \item $h(x)$ determines the distribution at $\theta = 0$
\end{itemize}

\TODO{more detail about the above terms. \eeg the $\theta$ are the sources or external
  fields.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Prob and stats}
\subsection{The Rules of Probability}
\begin{itemize}
  \item \tbf{Product Rule}: $p(x, y) = p(x|y) p(y) = p(y|x) p(x)$
  \item \tbf{Sum Rule}: $p(x) = \sum\limits_y p(x, y) = \sum\limits_y p(x | y) p(y)$
\end{itemize}

\subsection{Bayes' Rule}
Using $p(y|x) p(x) = p(x,y) = p(x|y) p(y)$, we have
\begin{equation}
  p(y|x) = \frac{p(x|y) p(y)}{p(x)} = \frac{p(x|y) p(y)}{\sum\limits_y p(x|y) p(y)}
\end{equation}

\subsection{Expectation and Variance}
\label{sec:ExpVar}
\begin{itemize}
  \item \tbf{Expectations of sum of variables add}:\\
  If $X_1, \dots, X_n$ are random variables, and $a_1, \dots, a_n$ are constants, then
  \begin{equation}
    \E \lp \sum_{i=1}^n a_i X_i \rp = \sum_{i=1}^n a_i\, \E (X_i)
  \end{equation}

  \item \tbf{Variances of sum of independent variables add}:\\
  If $X_1, \dots, X_n$ are \emph{independent} random variables, and $a_1, \dots, a_n$
  are constants, then
  \begin{equation}
    \trm{Var} \lp \sum_{i=1}^n a_i X_i \rp = \sum_{i=1}^n a_i^2\, \trm{Var} (X_i)
  \end{equation}

  \item \tbf{Variances of sum of (dependent) variables}:\\
  If $X$ and $Y$ are random variables, then
  \begin{align}
    \trm{Var} (X + Y) &= \trm{Var}(X) + \trm{Var}(Y) + 2 \trm{Cov}(X, Y) \nncr
    \trm{Var} (X - Y) &= \trm{Var}(X) + \trm{Var}(Y) - 2 \trm{Cov}(X, Y)
  \end{align}
  
\end{itemize}

\subsection{Central Limit Theorem}
Let $S_N = \sum_{i=1}^N X_i$, where each $X_i$ is \tbf{iid} with mean $\mu$ and variance
$\sigma^2$. Then, as $N \to \infty$, the pdf of $S_N$ approaches a normal distribution:
\begin{equation}
  p(S_N = s) = \frac{1}{\lp 2 \pi N \sigma^2 \rp^{1/2}} \exp \lb - \frac{(s - N
    \mu)^2}{2 N \sigma^2} \rb 
\end{equation}
NB the factors of $N$ in the pdf, which make the pdf mean/variance equal to $N$ times
the original mean/variance (\eie means and variances of independent variables add; see
section \ref{sec:ExpVar}.)


\subsection{(Weak) Law of Large Numbers}
Let $X_1, \dots, X_n$ be \tbf{iid}, and $\mu = \E(X_1)$\footnote{$\mu = \E(X_1) =
  \E(X_i)$ for any $1 \leq i \leq n$}. Defining the \tbfit{sample mean} as
$\overline{X}_n = n^{-1} \sum_{i=1}^n X_i$, the WLLN states that $\overline{X}_n$
converges in probability to $\mu$. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Information Theory}
\paragraph{KL divergence:}
\begin{align}
  KL\big[p(x) || q(x)\big]
  &= \sum_{x_i} p(x_i) \log \lp \frac{p(x_i)}{q(x_i)} \rp
    = -\sum_{x_i} p(x_i) \log \lp \frac{q(x_i)}{p(x_i)} \rp \nncr 
  &= -\sum_{x_i} p(x_i) \log q(x_i) + \sum_{x_i} p(x_i) \log p(x_i) \nncr 
  &= H(p,q) - H(p)
\end{align}
where $H(p,q)$ is the cross entropy, and $H(p)$ is the entropy.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bayesian}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Optimal Stopping Theory}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\printbibliography[heading=bibintoc]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-engine: xetex
%%% End:
