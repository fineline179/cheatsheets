%% Prob/Stats cheat sheet
%% (compile with XeTeX. see notes in structure_cheatSheet.tex if no XeTeX.)

\documentclass[11pt]{article}
\synctex=1
\input{structure_cheatSheet.tex}
\usepackage{xfrac}
\addbibresource{refs.bib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Prob/Stats Cheatsheet}
\author{Steve Young}
\abstract{Everything I know about prob/stats/maybe information theory too..}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conventions}
\paragraph{Math Notation}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Distributions}
\subsection{Gaussians}

\begin{enumerate}
  \item To start with, \emph{memorize} that
  \begin{equation}
    \boxed{\int_{-\infty}^{\infty} dx \, e^{-x^2} = \sqrt{\pi}}
  \end{equation}

  \item Next, anything multiplying the $x^2$ in the integrand is present in inverse
  under the square root.
  \begin{equation}
    \int_{-\infty}^{\infty} dx \, e^{-\trm{stuff}\, x^2} =
    \sqrt{\frac{\pi}{\trm{stuff}}}
  \end{equation}
  so, for example:
  \begin{equation}
    \int_{-\infty}^{\infty} dx \, e^{-\frac{1}{2} a x^2} =
    \sqrt{\frac{2 \pi}{a}}
  \end{equation}

\end{enumerate}

\subsection{Bernoulli}
For $x \in \{0, 1\}$, Bernoulli dist parametrized by $\mu$, with
\begin{equation}
  p(x; \mu) = \mu^x (1-\mu)^{1-x}
\end{equation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Prob and stats}
\subsection{The Rules of Probability}
\begin{itemize}
  \item \tbf{Product Rule}: $p(x, y) = p(x|y) p(y) = p(y|x) p(x)$
  \item \tbf{Sum Rule}: $p(x) = \sum\limits_y p(x, y) = \sum\limits_y p(x | y) p(y)$
\end{itemize}

\subsection{Bayes' Rule}
Using $p(y|x) p(x) = p(x, y) = p(x|y) p(y)$, we have
\begin{equation}
  p(y|x) = \frac{p(x|y) p(y)}{p(x)} = \frac{p(x|y) p(y)}{\sum\limits_y p(x|y) p(y)}
\end{equation}

\subsection{Covariance}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Information Theory}
\paragraph{KL divergence:}
\begin{align}
  KL\big[p(x) || q(x)\big] &= \sum_{x_i} p(x_i) \log\left(\frac{p(x_i)}{q(x_i)} \right)
                              = -\sum_{x_i} p(x_i) \log\left(\frac{q(x_i)}{p(x_i)}
                              \right) \nncr 
  &= -\sum_{x_i} p(x_i) \log q(x_i) + \sum_{x_i} p(x_i) \log p(x_i) \nncr
  &= H(p,q) - H(p)
\end{align}
where $H(p,q)$ is the cross entropy, and $H(p)$ is the entropy.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bayesian}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Optimal Stopping Theory}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\printbibliography[heading=bibintoc]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-engine: xetex
%%% End:
