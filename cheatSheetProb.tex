%% Prob/Stats cheat sheet
%% (compile with XeTeX. see notes in structure_cheatSheet.tex if no XeTeX.)

\documentclass[11pt]{article}
\synctex=1
\input{structure_cheatSheet.tex}
\usepackage{xfrac}
\addbibresource{refs_cheatSheet.bib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Prob/Stats Cheatsheet}
\author{Steve Young}
\abstract{Everything I know about prob/stats/maybe information theory too..}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conventions}
\paragraph{Math Notation}
\begin{itemize}
  \item $\mathbb{Z^+}$: positive integers
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Distributions}
\subsection{Gaussians}
\subsubsection{Basics}
\begin{enumerate}
  \item To start with, \emph{memorize} that
  \begin{equation}
    \boxed{\int_{-\infty}^{\infty} dx\, e^{-x^2} = \pi^{1/2}}
  \end{equation}

  \item Next, anything multiplying the $x^2$ in the integrand is present in inverse
  under the square root.
  \begin{equation}
    \int_{-\infty}^{\infty} dx\, e^{-\trm{stuff}\, x^2} =
    \lp \frac{\pi}{\trm{stuff}} \rp^{1/2}
  \end{equation}
  so, for example:
  \begin{equation}
    \label{eq:int_gauss_a2}
    \int_{-\infty}^{\infty} dx\, e^{-\frac{1}{2} a x^2} = \lp \frac{2 \pi}{a} \rp^{1/2} 
  \end{equation}

  \item The traditional Gaussian pdf (or \tbfit{Normal distribution}) has
  $a = 1 / \sigma^2$, and is easily seen to be
  \begin{equation}
    \mathcal{N}(x|0,\sigma^2) = \frac{1}{\lp 2 \pi \sigma^2 \rp^{1/2}}\,
    e^{-\frac{1}{2 \sigma^2} x^2}
  \end{equation}
\end{enumerate}

\begin{itemize}
  \item Normal distribution has ~67\% of pdf between mean \pm 1 std, ~95\% of pdf
  between mean \pm 2 std
\end{itemize}


\subsubsection{Differentiation moment trick}
\label{subsubsec:dmt}
By differentiating Eq. \ref{eq:int_gauss_a2} wrt $a$, we obtain an expression for
integrals of the form $\int_{-\infty}^{\infty}dx \, x^{2n} e^{-\frac{1}{2} a x^2}$, with
$n \in \mathbb{Z^+}$.

\eeg for $n=1$:
\begin{equation}
  - 2 \frac{d}{da} \int_{-\infty}^{\infty}dx \, e^{-\frac{1}{2} a x^2} =
  \int_{-\infty}^{\infty}dx \, x^2 e^{-\frac{1}{2} a x^2} =
  - 2 \frac{d}{da} \lp \frac{2\pi}{a} \rp^{1/2} =
  \lp \frac{2\pi}{a} \rp^{1/2} \, \frac{1}{a} 
\end{equation}
For $n=2$:
\begin{equation}
  \lp -2 \frac{d}{da} \rp^2 \int_{-\infty}^{\infty}dx \, e^{-\frac{1}{2} a x^2} =
  \int_{-\infty}^{\infty}dx \, x^4 e^{-\frac{1}{2} a x^2} =
  \lp -2 \frac{d}{da} \rp^2 \lp \frac{2\pi}{a} \rp^{1/2} =
  \lp \frac{2\pi}{a} \rp^{1/2} \, \frac{1}{a} \, \frac{3}{a}
\end{equation}
Generally:
\begin{equation}
  \int_{-\infty}^{\infty}dx \, x^{2n} e^{-\frac{1}{2} a x^2} =
  \lp \frac{2\pi}{a} \rp^{1/2} \frac{1}{a^n} (2n-1) (2n-3) \cdot\cdot\cdot 5 \cdot 3
  \cdot 1 
\end{equation}

We thus obtain an expression for the expectation value of $x^{2n}$ under the Gaussian
distribution: 
\begin{equation}
  \label{eq:x2n_moments}
  \langle x^{2n} \rangle =
  \frac{\int_{-\infty}^{\infty}dx \, x^{2n} e^{-\frac{1}{2} a x^2}}
       {\int_{-\infty}^{\infty}dx \, e^{-\frac{1}{2} a x^2}} =
  \frac{1}{a^n} (2n-1) (2n-3) \cdot\cdot\cdot 5 \cdot 3 \cdot 1
\end{equation}

\subsubsection{Gaussian with Linear Term}
To evaluate integrals of the form
\begin{equation}
  \label{eq:int_gauss_a2_j}
  \int_{-\infty}^{\infty} dx\, e^{-\frac{1}{2} a x^2 + J x},
\end{equation}
first complete the square in the exponent
\begin{equation}
  -\frac{a}{2} x^2 + J x = -\frac{a}{2} \lp x^2 - \frac{2Jx}{a} \rp =
  -\frac{a}{2} \lp x - \frac{J}{a} \rp^2 + \frac{J^2}{2a}
\end{equation}
which gives
\begin{equation}
  \int_{-\infty}^{\infty} dx\, e^{-\frac{1}{2} a x^2 + J x} =
  \int_{-\infty}^{\infty} dx\, e^{-\frac{1}{2} a\, \lp x - J/a \rp} e^{J^2 / 2a} =
  \lp\frac{2\pi}{a} \rp^{1/2} \, e^{J^2 / 2a}
\end{equation}
where the integral is done by shifting $x \to x + J a$ (or noting that the infinite
integral of a Gaussian is independent of its mean.)

By differentiating this expression wrt $J$ repeatedly, and finally setting $J=0$, we
obtain another way of deriving the moments of the Gaussian,
Eq. \eqref{eq:x2n_moments}. This motivates the introduction of the \tbfit{moment
  generating function}: given a pdf $p(x)$, the moment generating function is
\begin{equation}
  \psi_x(J) = \E_x \lb e^{J x} \rb = \int_{-\infty}^{\infty} dx\, e^{J x} p(x)
\end{equation}
which satisfies
\begin{equation}
  \la x^n \ra = \frac{d^n\, \psi_x(J)}{d J^n}\Bigg|_{J=0}
\end{equation}

% so that the moment generating function for the Gaussian distribution is
% \begin{equation}
%   \frac{1}{\lp 2 \pi \sigma^2 \rp^{1/2}}\, \int_{-\infty}^{\infty} dx\,
%   e^{J x}  e^{-\frac{1}{2 \sigma^2} x^2}, 
% \end{equation}



\TODO{Finish. Figure out clear way to include normalization factor of pdf in exposition}

\subsubsection{Multivariate Gaussians}
Promoting $a$ to a real $N \times N$ symmetric matrix $\mA$, and x and J to a $N$-dim
vectors $\v{x}$ and $\v{J}$ with components $x_i$ and $J_i$, we have the multivariate
Gaussian integral 
\begin{equation}
  \prod_{i=1}^N \lp \int_{-\infty}^{\infty} dx_i \rp
  e^{-\frac{1}{2} \v{x}^T \mA \v{x} + \v{J}^T \v{x}} =
  \lp \frac{(2 \pi)^{N/2}}{|\mA|^{1/2}} \rp e^{\frac{1}{2} \v{J}^T \mA^{-1} \v{J}}
\end{equation}
\TODOFIN{}

A detailed workthrough of multivariate Gaussian integrals is in
\href{http://vixra.org/abs/1404.0026}{\ttt{viXra:1404.0026}}.

\subsection{Bernoulli}
For $x \in \{0, 1\}$, Bernoulli dist parameterized by $\mu$ (prob of drawing $x=1$ is
$\mu$).
\begin{equation}
  p(x; \mu) = \mu^x (1-\mu)^{1-x}
\end{equation}

\subsection{Exponential Family}
These are pdfs of the form
\begin{equation}
  p(x;\theta) = h(x) \exp \lb \theta^T T(x) - A(\theta) \rb
\end{equation}
where
\begin{itemize}
  \item $\theta$ is the \tbfit{natural} or \tbfit{canonical parameter}
  \item $T(x)$ is the \tbfit{sufficient statistic}
  \item $A(\theta)$ is the \tbfit{log partition function}
  \item $h(x)$ determines the distribution at $\theta = 0$
\end{itemize}

\TODO{more detail about the above terms. \eeg the $\theta$ are the sources or external
  fields.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Prob and stats}
\subsection{The Rules of Probability}
\begin{itemize}
  \item \tbf{Product Rule}: $p(x, y) = p(x|y) p(y) = p(y|x) p(x)$
  \item \tbf{Sum Rule}: $p(x) = \sum\limits_y p(x, y) = \sum\limits_y p(x | y) p(y)$
\end{itemize}

\subsection{Bayes' Rule}
Using $p(y|x) p(x) = p(x,y) = p(x|y) p(y)$, we have
\begin{equation}
  p(y|x) = \frac{p(x|y) p(y)}{p(x)} = \frac{p(x|y) p(y)}{\sum\limits_y p(x|y) p(y)}
\end{equation}

\subsection{Expectation and Variance}
\label{sec:ExpVar}
\begin{itemize}
  \item \tbf{Expectations of sum of variables add}:\\
  If $X_1, \dots, X_n$ are random variables, and $a_1, \dots, a_n$ are constants, then
  \begin{equation}
    \E \lp \sum_{i=1}^n a_i X_i \rp = \sum_{i=1}^n a_i\, \E (X_i)
  \end{equation}

  \item \tbf{Variances of sum of independent variables add}:\\
  If $X_1, \dots, X_n$ are \emph{independent} random variables, and $a_1, \dots, a_n$
  are constants, then
  \begin{equation}
    \trm{Var} \lp \sum_{i=1}^n a_i X_i \rp = \sum_{i=1}^n a_i^2\, \trm{Var} (X_i)
  \end{equation}

  \item \tbf{Variances of sum of (dependent) variables}:\\
  If $X$ and $Y$ are random variables, then
  \begin{align}
    \trm{Var} (X + Y) &= \trm{Var}(X) + \trm{Var}(Y) + 2 \trm{Cov}(X, Y) \nncr
    \trm{Var} (X - Y) &= \trm{Var}(X) + \trm{Var}(Y) - 2 \trm{Cov}(X, Y)
  \end{align}
  
\end{itemize}

\subsection{(Weak) Law of Large Numbers (WLLN)}
\label{subsec:wlln}
\begin{itemize}
  \item Let $X_1, \dots, X_n$ be \tbf{iid}, each with mean $\mu$ and variance
  $\sigma^{2}$. Defining the \tbfit{sample mean} as
  $Y_n = \frac{1}{n} (X_1 + \dots + X_n)$, the WLLN states that $Y_n$ converges in
  probability to $\mu$. The variance of $Y_n$ goes to $\sigma^{2}/n$.

  \item The \textbf{practical application} of this is that we use the average of
  repeated samples of an \textbf{iid} variable to estimate the variable's \textbf{mean}.

\end{itemize}

\subsection{Central Limit Theorem}
Let $S_n = \sum_{i=1}^n X_i$, where each $X_i$ is \tbf{iid} with mean $\mu$ and variance
$\sigma^2$. The \tbfit{central limit theorem} says that as $n \to \infty$, the pdf of
$S_n$ approaches a normal distribution:
\begin{equation}
  p(S_n = s) = \frac{1}{\lp 2 \pi n \sigma^2 \rp^{1/2}} \exp \lb - \frac{(s - n
    \mu)^2}{2 n \sigma^2} \rb 
\end{equation}
NB the factors of $n$ in the pdf, which make the pdf mean/variance equal to $n$ times
the original mean/variance (\eie means and variances of independent variables add; see
section \ref{sec:ExpVar}.)

\textbf{Another interpretation}\footnote{From MIT finance 18.S096 course, probability
  lecture}: Compare to WLLN in section \ref{subsec:wlln}, where we had
$Y_n = \frac{1}{n} (X_1 + \dots + X_n)$, with variance $\sigma^2/n$. Now consider the
case where each $X_i$ has mean 0, and instead form the sum
$Z_n = \sqrt{n}\, Y_n = \frac{1}{\sqrt{n}} (X_1 + \dots + X_n)$. It follows that $Z_n$
has mean 0 and variance $\sigma^2$, and the central limit theorem says that $Z_n$ in
fact converges to $\mathcal{N}(0, \sigma^2)$ as $n \to \infty$.


\subsection{Statistical Tests of Hypotheses}
\TODO{Write this section. about how to do a/b tests. Stuff from inferentialthinking.com}

\begin{itemize}
  \item \textbf{total variation distance (between two categorical distributions)}:
  \item \tbfit{p-value}: The p-value of a test is the chance, based on the model in the
  null hypothesis, that the test statistic will be equal to the observed value in the
  sample or even further in the direction that supports the alternative


\end{itemize}

\subsubsection{A/B Testing}
For comparing two random samples (see if two samples come from same underlying
distribution).  \TODO{write this section}



\subsection{All About Regression}
\TODO{Write this section. All the relevant basics about linear regression (errors on fit
  coeffs, $R^2$ value, etc...)}


\subsection{Misc}

\subsubsection{Expected Number of Trials (Geometric Distribution)}
For Benoulli var which is 1 with prob $\mu$, what is expected number of independent draws
to get first 1?

\begin{align}
  p(\textrm{1 first occurs on $k$th draw}) &= (1-\mu)^{k-1} \mu \nncr
  \E[k] = \sum_{k=1}^{\infty} k \cdot p(\textrm{1 first occurs on $k$th draw}) &=
          \sum_{k=1}^{\infty} k (1-\mu)^{k-1} \mu = \frac{1}{\mu}
\end{align}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Information Theory}
\paragraph{KL divergence:}
\begin{align}
  KL\big[p(x) || q(x)\big]
  &= \sum_{x_i} p(x_i) \log \lp \frac{p(x_i)}{q(x_i)} \rp
    = -\sum_{x_i} p(x_i) \log \lp \frac{q(x_i)}{p(x_i)} \rp \nncr 
  &= -\sum_{x_i} p(x_i) \log q(x_i) + \sum_{x_i} p(x_i) \log p(x_i) \nncr 
  &= H(p,q) - H(p)
\end{align}
where $H(p,q)$ is the cross entropy, and $H(p)$ is the entropy.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bayesian}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\TODO{Remaining topics}}
\begin{itemize}
  \item Probability change of variables (with examples)
  \item Optimal Stopping Theory
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\printbibliography[heading=bibintoc]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-engine: xetex
%%% End:
