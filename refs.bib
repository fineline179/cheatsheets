@book{BISHOP,
 author = {Bishop, Christopher M.},
 title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
 year = {2006},
 isbn = {0387310738},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
 url = {https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/}
}

@book{PRESS,
 author = {Press, William H. and Teukolsky, Saul A. and Vetterling, William T. and
                  Flannery, Brian P.},
 title = {Numerical Recipes 3rd Edition: The Art of Scientific Computing},
 year = {2007},
 isbn = {0521880688, 9780521880688},
 edition = {3},
 publisher = {Cambridge University Press},
 address = {New York, NY, USA},
}

@book{RASMUSSEN,
 author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
 title = {Gaussian Processes for Machine Learning (Adaptive Computation and Machine
                  Learning)},
 year = {2005},
 isbn = {026218253X},
 publisher = {The MIT Press},
}

@misc{POOLE_2016,
    title={Exponential expressivity in deep neural networks through transient chaos},
    author={Ben Poole and Subhaneil Lahiri and Maithra Raghu and Jascha Sohl-Dickstein
                  and Surya Ganguli},
    year={2016},
    eprint={1606.05340},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@misc{SCHOENHOLZ_2016,
    title={Deep Information Propagation},
    author={Samuel S. Schoenholz and Justin Gilmer and Surya Ganguli and Jascha
                  Sohl-Dickstein},
    year={2016},
    eprint={1611.01232},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@misc{LEE_2017,
    title={Deep Neural Networks as Gaussian Processes},
    author={Jaehoon Lee and Yasaman Bahri and Roman Novak and Samuel S. Schoenholz and
                  Jeffrey Pennington and Jascha Sohl-Dickstein}, 
    year={2017},
    eprint={1711.00165},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@misc{DYER_2019,
    title={Asymptotics of Wide Networks from Feynman Diagrams},
    author={Ethan Dyer and Guy Gur-Ari},
    year={2019},
    eprint={1909.11304},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@incollection{CHO_2009,
    title = {Kernel Methods for Deep Learning},
    author = {Youngmin Cho and Lawrence K. Saul},
    booktitle = {Advances in Neural Information Processing Systems 22},
    editor = {Y. Bengio and D. Schuurmans and J. D. Lafferty and C. K. I. Williams and
                  A. Culotta},
    pages = {342--350},
    year = {2009},
    publisher = {Curran Associates, Inc.},
    url = {https://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning}
}

@misc{BELKIN_2018,
    title={Reconciling modern machine learning practice and the bias-variance trade-off},
    author={Mikhail Belkin and Daniel Hsu and Siyuan Ma and Soumik Mandal},
    year={2018},
    eprint={1812.11118},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@inbook{NEAL_1996,
    author="Neal, Radford M.",
    title="Priors for Infinite Networks",
    bookTitle="Bayesian Learning for Neural Networks",
    year="1996",
    publisher="Springer New York",
    address="New York, NY",
    pages="29--53",
    abstract="In this chapter, I show that priors over network parameters can be defined
                  in such a way that the corresponding priors over functions computed by
                  the network reach reasonable limits as the number of hidden units goes
                  to infinity. When using such priors,there is thus no need to limit the
                  size of the network in order to avoid ``overfitting''. The infinite
                  network limit also provides insight into the properties of different
                  priors. A Gaussian prior for hidden-to-output weights results in a
                  Gaussian process prior for functions,which may be smooth, Brownian, or
                  fractional Brownian. Quite different effects can be obtained using
                  priors based on non-Gaussian stable distributions. In networks with
                  more than one hidden layer, a combination of Gaussian and non-Gaussian
                  priors appears most interesting.",
    isbn="978-1-4612-0745-0",
    doi="10.1007/978-1-4612-0745-0_2",
    url="https://doi.org/10.1007/978-1-4612-0745-0_2"
}

@misc{JACOT_2018,
    title={Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
    author={Arthur Jacot and Franck Gabriel and Cl√©ment Hongler},
    year={2018},
    eprint={1806.07572},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{dyerAsymptoticsWideNetworks2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1909.11304},
  primaryClass = {hep-th, stat},
  title = {Asymptotics of {{Wide Networks}} from {{Feynman Diagrams}}},
  abstract = {Understanding the asymptotic behavior of wide networks is of considerable
                  interest. In this work, we present a general method for analyzing this
                  large width behavior. The method is an adaptation of Feynman diagrams,
                  a standard tool for computing multivariate Gaussian integrals. We
                  apply our method to study training dynamics, improving existing bounds
                  and deriving new results on wide network evolution during stochastic
                  gradient descent. Going beyond the strict large width limit, we
                  present closed-form expressions for higher-order terms governing wide
                  network training, and test these predictions empirically.},
  journal = {arXiv:1909.11304 [hep-th, stat]},
  author = {Dyer, Ethan and {Gur-Ari}, Guy},
  month = sep,
  year = {2019},
  keywords = {High Energy Physics - Theory,Statistics - Machine Learning,Computer
                  Science - Machine Learning},
  file = {/home/fineline/Dropbox/zoteroPDFs/2019 - Dyer_Gur-Ari - Asymptotics of Wide
                  Networks from Feynman
                  Diagrams.pdf;/home/fineline/Zotero/storage/754S7EHF/2019 -
                  Dyer_Gur-Ari - Asymptotics of Wide Networks from Feynman
                  Diagrams_ICML.pdf;/home/fineline/Zotero/storage/TKIZG2SH/1909.html}
}

@misc{LEE_2019,
    title={Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient
                  Descent},
    author={Jaehoon Lee and Lechao Xiao and Samuel S. Schoenholz and Yasaman Bahri and
                  Roman Novak and Jascha Sohl-Dickstein and Jeffrey Pennington},
    year={2019},
    eprint={1902.06720},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{THOOFT_1973,
      author         = "'t Hooft, Gerard",
      title          = "{A Planar Diagram Theory for Strong Interactions}",
      journal        = "Nucl. Phys.",
      volume         = "B72",
      year           = "1974",
      pages          = "461",
      doi            = "10.1016/0550-3213(74)90154-0",
      note           = "[,337(1973)]",
      reportNumber   = "CERN-TH-1786",
      SLACcitation   = "%%CITATION = NUPHA,B72,461;%%"
}

